{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbc075d-a2ff-4f12-86d9-4b3c71331a35",
   "metadata": {},
   "source": [
    "# Aarib Ahmed Vahidy 22K-4004 BAI-6A\n",
    "\n",
    "## Information Retrieval Assignment # 1\n",
    "\n",
    "### Inverted Index, Positional Index, Boolean Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4f8ad-801e-420e-883b-673ba7b55604",
   "metadata": {},
   "source": [
    "Assignment Objective\n",
    "The objective of this assignment is to make you understand how different indexes work in\n",
    "retrieving different queries from a collection. You will create an Inverted index and Positional\n",
    "index for a set of collection to facilitate the Boolean Model of IR. Inverted files and Positional\n",
    "files are the primary data structures to support the efficient determination of which documents\n",
    "contain specified terms and at which proximity. You also learn to process simple Boolean\n",
    "expression queries through this assignment.\n",
    "Datasets\n",
    "You are given a collection of Abstracts (File name: Abstracts.zip) for implementing inverted index\n",
    "and positional index. This zip file contains 448 abstracts of some computer science journal. The\n",
    "language of all these documents is English. You also need to implement a pre-processing pipeline.\n",
    "It is recommended to first review the given text file for indexing. You need to treat each document\n",
    "as a unique document. This observation offers you many clues for your pipeline implementation\n",
    "and feature extraction.\n",
    "Query Processing\n",
    "In this assignment, all you need to implement an information retrieval model called Boolean\n",
    "Information Retrieval Model with some simplified assumptions. You need to treat each abstract\n",
    "(document or file as a document and need to index it content separately. you need to implement a\n",
    "simplified Boolean user query that can only be formed by joining three terms (t1, t2 and t3) with\n",
    "(AND, OR, and NOT) Boolean operators. For example, a user query may be in the form (t1 AND\n",
    "t2 AND t3). For positional queries, the query text contains “/” along with a k intended to return\n",
    "all documents that contain t1 and t2, k words apart on either side of the text.\n",
    "Basic Assumption for Boolean Retrieval Model\n",
    "1. An index term (word) is either present (1) or absent (0) in the document. A dictionary\n",
    "contains all index terms.\n",
    "2. All index terms provide equal evidence with respect to information needs. (No frequency\n",
    "count necessary, but in next assignment it can be)\n",
    "3. Queries are Boolean combinations of index terms (at max 3).\n",
    "4. Boolean Operators (AND, OR and NOT) are allowed. For examples:\n",
    "X AND Y: represents doc that contains both X and Y\n",
    "X OR Y: represents doc that contains either X or Y\n",
    "NOT X: represents the doc that do not contain X\n",
    "5. Queries of the type X Y / 3 represents doc that contains both X and Y and 3 words apart.\n",
    "\n",
    "As we discussed during the lectures, we will implement a Boolean Model by creating a posting\n",
    "list of all the terms present in the documents. You are free to implement a posting list with your\n",
    "choice of data structures; you are only allowed to preprocess the text from the documents in term\n",
    "of tokenization in which you can do case folding and stop-words removal and stemming. The stop\n",
    "word list is also provided to you in assignments files. Your query processing routine must address\n",
    "a query parsing, evaluation of the cost, and through executing it to fetch the required list of\n",
    "documents. A command line interface is simply required to demonstrate the working model. You\n",
    "are also provided by a set of 10 queries, for evaluating your implementation.\n",
    "Coding can be done in either Java, Python, C/C++ or C# programming language. There are\n",
    "additional marks for intuitive GUI for demonstrating the working Boolean Model along with\n",
    "phrase query search.\n",
    "Files Provided with this Assignment:\n",
    "1. Abstracts\n",
    "2. Stop-words list as a single file\n",
    "3. Queries Result-set (Gold Standard- 10 example queries)\n",
    "Evaluation/ Grading Criteria\n",
    "The grading will be done as per the scheme of implementations, query responses and matching\n",
    "with a gold standard (provided query set).\n",
    "Grading Criteria:\n",
    "Preprocessing (2 marks)\n",
    "Formation of Inverted and Positional Indexes (1 mark for code complexity 1 mark for saving and\n",
    "loading the indexes)\n",
    "Simple Boolean Queries (2 marks)\n",
    "Complex Boolean Queries (2 marks)\n",
    "Proximity Queries (2 marks)\n",
    "Bonus: GUI (1 mark for making the GUI 1 mark for good friendly GUI)\n",
    "The proper clean and well commented code will get 2 more marks.\n",
    "\n",
    "<The End>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc4a33-f26f-4199-94ad-b445f2a278bc",
   "metadata": {},
   "source": [
    "## Loading the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "15eaa6cb-feec-42d9-994b-5dc1b4d8e988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries have been installed successfully\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "print(\"All libraries have been installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28e96825-e669-4014-ac60-da8cc12222fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aarib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aarib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e6d28eb0-c0fa-497c-9667-b2ae06835b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.txt', '10.txt', '100.txt', '101.txt', '102.txt']\n"
     ]
    }
   ],
   "source": [
    "#Setting dataset path and checking the abstracts.rar files\n",
    "dataset_path = r\"C:\\Users\\aarib\\6thSemester\\IRAssignment\\Dataset\\Abstracts\"\n",
    "\n",
    "print(os.listdir(dataset_path)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5513f47d-39a1-48e8-b916-bce4cbcb3071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Statistical and Heuristic Models for Unsupervised Word Alignment\n",
      "\n",
      "statistical word alignmen\n"
     ]
    }
   ],
   "source": [
    "sample_file = os.listdir(dataset_path)[0]  #Picking the first file\n",
    "sample_path = os.path.join(dataset_path, sample_file)\n",
    "\n",
    "with open(sample_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "print(content[:100])  #Only printing first 100 characters of first file to just check and avoid clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c4694a7-5804-472c-8e57-33e37c0cd208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents loaded: 448\n",
      "First Sample document:\n",
      " Ensemble Statistical and Heuristic Models for Unsupervised Word Alignment\n",
      "\n",
      "statistical word alignment, ensemble learning, heuristic word alignment\n",
      "\n",
      "Statistical word alignment models need large amount of training data while they are weak in small-size corpora. This paper proposes a new approach of unsupervised hybrid word alignment technique using ensemble learning method. This algorithm uses three base alignment models in several rounds to generate alignments. The ensemble algorithm uses a weighed scheme for resampling training data and a voting score to consider aggregated alignments. The underlying alignment algorithms used in this study include IBM Model 1, 2 and a heuristic method based on Dice measurement. Our experimental results show that by this approach, the alignment error rate could be improved by at least %15 for the base alignment models.\n"
     ]
    }
   ],
   "source": [
    "#List to store document contents which were provided in the abstracts.rar file.\n",
    "documents = []\n",
    "\n",
    "#Reading each .txt file in the folder\n",
    "for filename in os.listdir(dataset_path): #dataset_path defined above\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"windows-1252\") as file: \n",
    "            #encoding = \"utf-8\" did not work so tried \"latin-1\" which worked but \"windows-1252\" worked best according to the documents given. \n",
    "            documents.append(file.read())\n",
    "\n",
    "#Checking if loading was a success and the number of documents loaded\n",
    "print(f\"Total documents loaded: {len(documents)}\")\n",
    "print(\"First Sample document:\\n\", documents[0][:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3aa377-997a-4b2d-ba5f-088b463a2cbe",
   "metadata": {},
   "source": [
    "## Preprocessing the Documents\n",
    "\n",
    "1. **Tokenization** (Splitting text into words)\n",
    "2. **Special Character Handling** (Replacing special characters such as /,_ with a space ' ' and storing hyphenated words both separately and in base form)\n",
    "3. **Case Folding** (Converting text into lowercase)  \n",
    "4. **Stop word removal** (Removing common words)  \n",
    "   Stop words list provided: *a, is, the, of, all, and, to, can, be, as, once, for, at, am, are, has, have, had, up, his, her, in, on, no, we, do*  \n",
    "5. **Stemming** (Reducing words to their base form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b3bd4e56-0ce2-4165-99ae-dd961ef65962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom stop words as provided by Stopword-list.txt\n",
    "my_stopwords = set([\n",
    "    \"a\", \"is\", \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \n",
    "    \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \n",
    "    \"in\", \"on\", \"no\", \"we\", \"do\"\n",
    "])\n",
    "\n",
    "#Initializing Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Dictionary to store the processed documents\n",
    "processed_docs = {} #It will store filename as key and preprocessed word list as the value.\n",
    "\n",
    "#Reading and processing each document\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"windows-1252\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        #Handling hyphenated words:\n",
    "        #Keeping the original word (e.g., \"time-series\").\n",
    "        #Replacing hyphens with spaces to store individual words (e.g., \"time series\").\n",
    "        #This preprocessing is necessary so that my results match those of the golden set. e.g in query 4\n",
    "        text = text.replace(\"-\", \" \") + \" \" + text  # Ensures both variants are indexed.\n",
    "\n",
    "         #Handling special characters (/, _, etc.) Can add more to regex if needed but I think these two are enough\n",
    "        #Replacing '/', '_' with spaces to split words properly\n",
    "        #This preprocessing is necessary so that my results match those of the golden set. e.g in query 9\n",
    "        text = re.sub(r'[/_]', ' ', text)  #Converts \"classification/clustering\" → \"classification clustering\"\n",
    "        \n",
    "        #Tokenization (SPlits the text into individual words)\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        #Case Folding (Converts all words to their lowercase equivalent.)\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        \n",
    "        #Stop-word Removal (Only considers those words which are not in out stopwords list)\n",
    "        filtered_tokens = [word for word in tokens if word not in my_stopwords]\n",
    "        \n",
    "        #Stemming (Stems words to their root forms for example running becomes run)\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "        \n",
    "        #Store processed document as key value pair where filename is key and list of processed words in value.\n",
    "        processed_docs[filename] = stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269fe6e0-31ac-4dd8-bb8b-8b3d93714a94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Checking if my preprocessing was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f693e9d-c71a-453e-b475-60f35e99041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 448 documents successfully!\n",
      "Sample processed document (1.txt):\n",
      " ['ensembl', 'statist', 'heurist', 'model', 'unsupervis', 'word', 'align', 'statist', 'word', 'align', ',', 'ensembl', 'learn', ',', 'heurist', 'word', 'align', 'statist', 'word', 'align', 'model', 'need', 'larg', 'amount', 'train', 'data', 'while', 'they', 'weak', 'small', 'size', 'corpora', '.', 'thi', 'paper', 'propos', 'new', 'approach', 'unsupervis', 'hybrid', 'word', 'align', 'techniqu', 'use', 'ensembl', 'learn', 'method', '.', 'thi', 'algorithm', 'use', 'three', 'base', 'align', 'model', 'sever', 'round', 'gener', 'align', '.', 'ensembl', 'algorithm', 'use', 'weigh', 'scheme', 'resampl', 'train', 'data', 'vote', 'score', 'consid', 'aggreg', 'align', '.', 'underli', 'align', 'algorithm', 'use', 'thi', 'studi', 'includ', 'ibm', 'model', '1', ',', '2', 'heurist', 'method', 'base', 'dice', 'measur', '.', 'our', 'experiment', 'result', 'show', 'that', 'by', 'thi', 'approach', ',', 'align', 'error', 'rate', 'could', 'improv', 'by', 'least', '%', '15', 'base', 'align', 'model', '.', 'ensembl', 'statist', 'heurist', 'model', 'unsupervis', 'word', 'align', 'statist', 'word', 'align', ',', 'ensembl', 'learn', ',', 'heurist', 'word', 'align', 'statist', 'word', 'align', 'model', 'need', 'larg', 'amount', 'train', 'data', 'while', 'they', 'weak', 'small-siz', 'corpora', '.', 'thi', 'paper', 'propos', 'new', 'approach', 'unsupervis', 'hybrid', 'word', 'align', 'techniqu', 'use', 'ensembl', 'learn', 'method', '.', 'thi', 'algorithm', 'use', 'three', 'base', 'align', 'model', 'sever', 'round', 'gener', 'align', '.', 'ensembl', 'algorithm', 'use', 'weigh', 'scheme', 'resampl', 'train', 'data', 'vote', 'score', 'consid', 'aggreg', 'align', '.', 'underli', 'align', 'algorithm', 'use', 'thi', 'studi', 'includ', 'ibm', 'model', '1', ',', '2', 'heurist', 'method', 'base', 'dice', 'measur', '.', 'our', 'experiment', 'result', 'show', 'that', 'by', 'thi', 'approach', ',', 'align', 'error', 'rate', 'could', 'improv', 'by', 'least', '%', '15', 'base', 'align', 'model', '.']\n"
     ]
    }
   ],
   "source": [
    "#Displaying a sample to check\n",
    "print(f\"Processed {len(processed_docs)} documents successfully!\")\n",
    "sample_doc = list(processed_docs.keys())[0]\n",
    "print(f\"Sample processed document ({sample_doc}):\\n\", processed_docs[sample_doc][:1500]) #Displaying first 1500 words to check, can also compare with old 1500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7eb6041-9e77-4e63-88d9-7af357cb25e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample processed document (223.txt):\n",
      " ['neural', 'network', 'base', 'kidney', 'segment', 'from', 'mr', 'imag', ':', 'preliminari', 'result', 'kidney', 'segment', ',', 'neural', 'network', ',', 'mr', 'imag', 'autom', 'robust', 'kidney', 'segment', 'from', 'medic', 'imag', 'sequenc', 'veri', 'difficult', 'task', 'particularli', 'becaus', 'gray', 'level', 'similar', 'adjac', 'organ', ',', 'partial', 'volum', 'effect', 'inject', 'contrast', 'media', '.', 'addit', 'these', 'difficulti', ',', 'variat', 'kidney', 'shape', ',', 'posit', 'gray', 'level', 'make', 'autom', 'identif', 'segment', 'kidney', 'harder', '.', 'also', ',', 'differ', 'imag', 'characterist', 'with', 'differ', 'scanner', 'much', 'more', 'increas', 'difficulti', 'segment', 'task', '.', 'therefor', ',', 'thi', 'paper', ',', 'present', 'an', 'autom', 'kidney', 'segment', 'method', 'by', 'use', 'multi', 'layer', 'perceptron', 'base', 'approach', 'that', 'adapt', 'paramet', 'accord']\n"
     ]
    }
   ],
   "source": [
    "#Checking random documents to make sure the preprocessing worked.\n",
    "import random\n",
    "sample_doc = random.choice(list(processed_docs.keys()))\n",
    "print(f\"Sample processed document ({sample_doc}):\\n\", processed_docs[sample_doc][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2add4-d7a4-44b1-9d39-e8315f95003f",
   "metadata": {},
   "source": [
    "## Building the Positings List (Inverted Index)\n",
    "\n",
    "DataStructure Used: Dictionary of Lists (Hashmap) O(1) time complexity for insertion and searching\n",
    "\n",
    "The keys are unique words (terms from documents)\n",
    "\n",
    "The values are lists of document filenames where the word appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76ae9e72-e54f-41aa-be97-73358d7f02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to store the inverted index/posting list\n",
    "posting_list = {}\n",
    "\n",
    "#Constructing the posting list\n",
    "for filename, words in processed_docs.items():\n",
    "    doc_id = filename  #Using the given filenames as the documentID\n",
    "    for word in set(words):  # Using a set to avoid duplicate entries\n",
    "        if word not in posting_list:\n",
    "            posting_list[word] = set()  #Using a set to store unique doc IDs, will only make new posting list if word is new, not encountered before.\n",
    "        posting_list[word].add(doc_id) #Adding the document ID where word appeared to the posting list for that word.\n",
    "\n",
    "#Converting sets to lists for better readability\n",
    "for word in posting_list:\n",
    "    posting_list[word] = sorted(list(posting_list[word]))  #Sorting based on documnet IDs for better consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d5d94c9-e964-476e-b448-e07641baa8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Posting List Entry: 'statist' appears in: ['1.txt', '102.txt', '112.txt', '115.txt', '116.txt', '121.txt', '128.txt', '14.txt', '145.txt', '147.txt', '15.txt', '156.txt', '158.txt', '17.txt', '170.txt', '190.txt', '193.txt', '194.txt', '202.txt', '204.txt', '208.txt', '228.txt', '255.txt', '269.txt', '283.txt', '319.txt', '320.txt', '336.txt', '343.txt', '355.txt', '368.txt', '370.txt', '385.txt', '393.txt', '405.txt', '41.txt', '42.txt', '429.txt', '43.txt', '430.txt', '438.txt', '445.txt', '447.txt', '448.txt', '60.txt', '71.txt', '72.txt', '92.txt']\n"
     ]
    }
   ],
   "source": [
    "#Checking for a specific word, e.g., \"statist\" (which was \"statistical\" before preprocessing)\n",
    "sample_term = \"statist\"\n",
    "if sample_term in posting_list:\n",
    "    print(f\"Sample Posting List Entry: '{sample_term}' appears in: {posting_list[sample_term]}\")\n",
    "else:\n",
    "    print(f\"The term '{sample_term}' is not found in the posting list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd4a5489-3fc8-4f8a-a556-9b6f3c02c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inverted Index was saved successfully\n"
     ]
    }
   ],
   "source": [
    "#Saving the inverted index/posting list\n",
    "with open(\"inverted_index.json\", \"w\") as file:\n",
    "    json.dump(posting_list, file, indent=4)\n",
    "print(\"The inverted Index was saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "345ac222-24de-4bc0-afa2-7b35a1c2b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Inverted Index was loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#Loading the previously saved inverted index\n",
    "with open(\"inverted_index.json\", \"r\") as file:\n",
    "    loaded_inverted_index = json.load(file)\n",
    "\n",
    "print(\"The Inverted Index was loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d032d568-da16-4add-ba65-e9e822bc6a93",
   "metadata": {},
   "source": [
    "## Building the Positional Index \n",
    "\n",
    "DataStructure Used: Nested Dictionary of Lists (HashMap) O(1) time complexity for insertion and searching\n",
    "\n",
    "The outer dictionary's key is a unique word\n",
    "\n",
    "The inner dictionary's key is the document ID (filename.txt)\n",
    "\n",
    "The inner dictionary's value is a list of positions where the word appears in that document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0ce1638f-2f81-47bb-8a56-e79b3b1d58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to store the positional index\n",
    "positional_index = {}\n",
    "\n",
    "#Constructing the positional index\n",
    "for filename, words in processed_docs.items():\n",
    "    doc_id = filename #Using the given filename as the documentID\n",
    "    for position, word in enumerate(words): #Tracking the position of each word\n",
    "        if word not in positional_index:\n",
    "            positional_index[word] = {} #Creating a new dictionary for each word\n",
    "\n",
    "        if doc_id not in positional_index[word]:\n",
    "            positional_index[word][doc_id] = [] #Creating a list for positions\n",
    "            \n",
    "        positional_index[word][doc_id].append(position) #Storing the position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4389dea-498e-4e41-8e02-769edccd582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positional Index Entry for 'statist':\n",
      "Document 1.txt: Positions [1, 7, 17, 115, 121, 131]\n",
      "Document 102.txt: Positions [16, 100, 108, 141, 225, 233]\n",
      "Document 112.txt: Positions [140, 337]\n",
      "Document 115.txt: Positions [0, 87]\n",
      "Document 116.txt: Positions [0, 5, 146, 151]\n",
      "Document 121.txt: Positions [131, 192, 355, 416]\n",
      "Document 128.txt: Positions [116, 144, 276, 304]\n",
      "Document 14.txt: Positions [56, 165]\n",
      "Document 145.txt: Positions [84, 272]\n",
      "Document 147.txt: Positions [8, 191]\n",
      "Document 15.txt: Positions [2, 4, 21, 96, 114, 116, 133, 206]\n",
      "Document 156.txt: Positions [0, 17, 60, 244, 261, 304]\n",
      "Document 158.txt: Positions [42, 219]\n",
      "Document 17.txt: Positions [60, 173]\n",
      "Document 170.txt: Positions [149, 357]\n",
      "Document 190.txt: Positions [49, 266]\n",
      "Document 193.txt: Positions [176, 406]\n",
      "Document 194.txt: Positions [51, 71, 209, 229]\n",
      "Document 202.txt: Positions [100, 226]\n",
      "Document 204.txt: Positions [0, 17, 96, 129, 145, 162, 241, 270]\n",
      "Document 208.txt: Positions [115, 187, 310, 377]\n",
      "Document 228.txt: Positions [51, 210]\n",
      "Document 255.txt: Positions [49, 159]\n",
      "Document 269.txt: Positions [158, 324]\n",
      "Document 283.txt: Positions [145, 350]\n",
      "Document 319.txt: Positions [113, 299]\n",
      "Document 320.txt: Positions [21, 75, 119, 127, 132, 184, 238, 282, 290, 295]\n",
      "Document 336.txt: Positions [101, 137, 256, 291]\n",
      "Document 343.txt: Positions [92, 213]\n",
      "Document 355.txt: Positions [167, 177, 374, 384]\n",
      "Document 368.txt: Positions [108, 266]\n",
      "Document 370.txt: Positions [99, 267]\n",
      "Document 385.txt: Positions [119, 268]\n",
      "Document 393.txt: Positions [148, 302]\n",
      "Document 405.txt: Positions [24, 42, 81, 176, 194, 233]\n",
      "Document 41.txt: Positions [15, 143, 165, 293]\n",
      "Document 42.txt: Positions [40, 216]\n",
      "Document 429.txt: Positions [98, 202]\n",
      "Document 43.txt: Positions [115, 306]\n",
      "Document 430.txt: Positions [20, 116, 120, 132, 136, 173, 269, 273, 285, 289]\n",
      "Document 438.txt: Positions [200, 404]\n",
      "Document 445.txt: Positions [34, 245]\n",
      "Document 447.txt: Positions [127, 365]\n",
      "Document 448.txt: Positions [23, 42, 163, 215, 234, 354]\n",
      "Document 60.txt: Positions [8, 58]\n",
      "Document 71.txt: Positions [50, 169]\n",
      "Document 72.txt: Positions [71, 199]\n",
      "Document 92.txt: Positions [18, 58, 162, 202]\n"
     ]
    }
   ],
   "source": [
    "#Checking for specific word for example statist (statistical before processing)\n",
    "sample_term = \"statist\"\n",
    "if sample_term in positional_index:\n",
    "    print(f\"\\nPositional Index Entry for '{sample_term}':\")\n",
    "    for doc, positions in positional_index[sample_term].items():\n",
    "        print(f\"Document {doc}: Positions {positions}\")\n",
    "else:\n",
    "    print(f\"\\nThe term '{sample_term}' is not found in the positional index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14ca255d-ff2e-4c8f-8964-97586659cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Positional Index was saved successfully\n"
     ]
    }
   ],
   "source": [
    "#Saving the Positional Index\n",
    "with open(\"positional_index.json\", \"w\") as file:\n",
    "    json.dump(positional_index, file, indent=4)\n",
    "print(\"The Positional Index was saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "afcaa621-edbe-4cec-a147-9cf0f90b3cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Inverted Index was loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "#Loading the previously saved positional index\n",
    "with open(\"positional_index.json\", \"r\") as file:\n",
    "    loaded_positional_index = json.load(file)\n",
    "print(\"The Inverted Index was loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc277bbd-5b11-4085-9d59-85e0efd23ac7",
   "metadata": {},
   "source": [
    "## Implementing Simple Boolean Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe36d4ad-fe05-4607-908d-5759448eb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(term1, operator1=None, term2=None, operator2=None, term3=None):\n",
    "    \"\"\"Processes Boolean queries with up to three terms using the posting list.\n",
    "        We will need to stem the words in the query so they match the processed words in postings list.\n",
    "        Supports single-term, two-term, and three-term queries.\n",
    "        if we have three terms we will need to perform 2 operations on the three terms.\n",
    "        The .txt is removed and sorted integer list of documentIDS is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    #Stemming the received terms so they match the postings format.\n",
    "    term1 = stemmer.stem(term1)\n",
    "    term2 = stemmer.stem(term2) if term2 else None\n",
    "    term3 = stemmer.stem(term3) if term3 else None  \n",
    "\n",
    "    #Universal Set of all document IDs\n",
    "    all_docs = set(doc for docs in posting_list.values() for doc in docs)\n",
    "\n",
    "    #Fetching the posting lists for the terms\n",
    "    docs_term1 = set(posting_list.get(term1, []))\n",
    "    docs_term2 = set(posting_list.get(term2, [])) if term2 else set() #Treating none case as an empty set\n",
    "    docs_term3 = set(posting_list.get(term3, [])) if term3 else set()\n",
    "\n",
    "    #Handling a single-term query\n",
    "    if not operator1:\n",
    "        return sorted([int(doc.replace('.txt', '')) for doc in docs_term1])\n",
    "\n",
    "    #Performing the first operation (between term1 and term2) and getting an interim result\n",
    "    if operator1 == \"AND\":\n",
    "        result1 = docs_term1 & docs_term2 #Boolean AND (Intersection)\n",
    "    elif operator1 == \"OR\":\n",
    "        result1 = docs_term1 | docs_term2#Boolean OR (Union)\n",
    "    elif operator1 == \"NOT\":\n",
    "        result1 = all_docs - docs_term1  #Boolean NOT (Complement using Universal Set)\n",
    "    else:\n",
    "        print(\"The entered operator1 is not valid boolean, use 'AND'/'OR'/'NOT'\")\n",
    "        return []\n",
    "\n",
    "    #If only two terms were provided, we will directly return result1\n",
    "    if not operator2:\n",
    "        return sorted([int(doc.replace('.txt', '')) for doc in result1])\n",
    "\n",
    "    #Performing the second operation(between result1 which was computed and term3 which was passed to the function) and getting the final result\n",
    "    if operator2 == \"AND\":\n",
    "        final_result = result1 & docs_term3 #Boolean AND (Intersection)\n",
    "    elif operator2 == \"OR\":\n",
    "        final_result = result1 | docs_term3 #Boolean OR (Union)\n",
    "    elif operator2 == \"NOT\":\n",
    "        final_result = result1 - docs_term3#Boolean NOT (Exclusion)\n",
    "    else:\n",
    "        print(\"The entered operator2 is not valid boolean, use 'AND'/'OR'/'NOT'\")\n",
    "        return []\n",
    "\n",
    "    return sorted([int(doc.replace('.txt', '')) for doc in final_result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a02ee5-fff6-4130-9751-f9c47e9f0db5",
   "metadata": {},
   "source": [
    "### Checking the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6c1e7277-8d31-41fd-baf9-799d315d57d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(boolean_query(\"statist\", \"AND\", \"word\", \"AND\", \"align\")) #Performing the stemming myself\n",
    "print(boolean_query(\"statistical\", \"AND\", \"word\", \"AND\", \"alignment\")) #Sending the natural words to check if function stems and outputs match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "56c53d34-560b-4312-b623-6119be81bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 12, 35, 46, 67, 84, 107, 117, 122, 123, 124, 125, 126, 128, 137, 155, 161, 165, 167, 169, 171, 187, 198, 216, 237, 240, 280, 283, 285, 289, 291, 292, 305, 319, 321, 368, 380, 400, 427, 428, 439, 443, 447]\n"
     ]
    }
   ],
   "source": [
    "print(boolean_query(\"mixture\", \"OR\", \"gamma\", \"OR\", \"svm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e86b33f1-c162-4acf-896a-1f93e61cbf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 17, 42, 106, 136, 240, 268, 287, 353, 354]\n"
     ]
    }
   ],
   "source": [
    "print(boolean_query(\"dimensionality\", \"AND\", \"reduction\", \"NOT\", \"security\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "705b26ac-ac77-48cf-9f37-0f0fab8a5f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 12, 67, 84, 107, 122, 123, 124, 125, 126, 128, 137, 155, 161, 167, 169, 171, 237, 280, 283, 285, 291, 292, 305, 319, 321, 324, 368, 400, 427, 428, 437, 439, 447]\n"
     ]
    }
   ],
   "source": [
    "print(boolean_query(\"housing\", \"AND\", \"pricing\", \"OR\", \"svm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528ddac-d194-4999-a54d-fac9659350d5",
   "metadata": {},
   "source": [
    "## Checking Golden Queries for Simple Boolean Queries (Query Number 1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d3ba3b3-16e9-4f1c-9778-83a6cdf61441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [359, 375]\n"
     ]
    }
   ],
   "source": [
    "#1.\n",
    "print(f\"Result Set: {boolean_query(\"image\", \"AND\", \"restoration\")}\") #Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2fec3ce-f65a-459e-bc1b-62fa0ecf5fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [23, 24, 174, 175, 176, 177, 213, 245, 247, 250, 254, 258, 267, 272, 273, 278, 279, 281, 325, 333, 345, 346, 347, 348, 352, 357, 358, 360, 362, 371, 373, 374, 375, 380, 381, 382, 396, 397, 401, 404, 405, 415, 421, 432, 444]\n"
     ]
    }
   ],
   "source": [
    "#2.\n",
    "print(f\"Result Set: {boolean_query(\"deep\", \"AND\", \"learning\")}\") #Not matching, learning gets stemmed to learn and matches more.\n",
    "#176 should get matched as it contains both deep and learning but it is not included in the golden set, dont understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0f25607-1534-44f6-9200-8f15b7bf15ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [187, 273, 279, 325, 333, 405]\n"
     ]
    }
   ],
   "source": [
    "#3.\n",
    "print(f\"Result Set: {boolean_query(\"autoencoders\")}\") #Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "527a5a3d-98e8-4116-a0b6-618a046056fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [279, 358, 373, 405]\n"
     ]
    }
   ],
   "source": [
    "#4.\n",
    "print(f\"Result Set: {boolean_query(\"temporal\", \"AND\", \"deep\", \"AND\", \"learning\")}\") #Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "08fa906b-1585-4ce3-8547-5b7843dc3429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [40, 54, 110, 111, 112, 113, 158, 163, 173, 180, 181, 202, 220, 237, 238, 239, 240, 258, 277, 283, 295, 305, 350, 405, 421, 437, 438, 445]\n"
     ]
    }
   ],
   "source": [
    "#5.\n",
    "print(f\"Result Set: {boolean_query(\"time\", \"AND\", \"series\")}\") #Matching, required handling hyphenated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff603452-2cd6-40cc-a36a-46c2af1a185b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [40, 237, 283, 445]\n"
     ]
    }
   ],
   "source": [
    "#6.\n",
    "print(f\"Result Set: {boolean_query(\"time\", \"AND\", \"series\", \"AND\", \"classification\")}\") #Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e54aa8d-c03e-487e-8cde-94392f1b4543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [4, 6, 9, 10, 16, 22, 24, 33, 34, 38, 40, 43, 45, 46, 49, 51, 54, 55, 56, 58, 59, 60, 63, 64, 66, 67, 71, 73, 75, 76, 77, 80, 84, 85, 94, 95, 98, 99, 106, 107, 110, 111, 112, 113, 120, 121, 122, 123, 125, 126, 128, 140, 143, 147, 158, 163, 164, 165, 167, 168, 169, 171, 173, 174, 175, 176, 177, 180, 181, 182, 187, 193, 197, 198, 202, 208, 210, 213, 215, 220, 228, 229, 234, 235, 236, 237, 238, 239, 240, 245, 247, 248, 249, 252, 256, 258, 259, 261, 265, 268, 272, 273, 277, 280, 283, 286, 287, 289, 295, 299, 302, 303, 305, 310, 313, 316, 317, 321, 327, 328, 334, 338, 341, 345, 348, 350, 352, 353, 354, 357, 363, 369, 371, 375, 377, 378, 382, 384, 385, 386, 387, 395, 397, 404, 405, 408, 420, 421, 424, 425, 427, 432, 437, 438, 439, 442, 445]\n"
     ]
    }
   ],
   "source": [
    "#7.\n",
    "print(f\"Result Set: {boolean_query(\"time\", \"AND\", \"series\", \"OR\", \"classification\")}\") #Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e8ad5dc-cd24-4094-ac22-c97cedec7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [9, 10, 18, 21, 23, 26, 30, 34, 40, 50, 73, 118, 126, 127, 139, 145, 148, 155, 180, 186, 189, 194, 201, 209, 214, 216, 230, 231, 234, 238, 279, 280, 288, 326, 343, 350, 351, 368, 369, 383, 394, 406, 412, 413, 424, 425, 429, 446, 447]\n"
     ]
    }
   ],
   "source": [
    "#8.\n",
    "print(f\"Result Set: {boolean_query(\"pattern\")}\") #Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb344a2d-0fbf-4054-bcaa-def5ca1b7830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [40, 73, 180, 216, 326, 350, 351, 413, 446]\n"
     ]
    }
   ],
   "source": [
    "#9.\n",
    "print(f\"Result Set: {boolean_query(\"pattern\", \"AND\", \"clustering\")}\") \n",
    "#40 missing as it contains \"Classification/Clustering\" which is being treated as one token so clustering is not being indexed\n",
    "#We need to improve preprocessing to handle '/' as well. After dealing with /, 40 matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e34e0449-7f78-4a18-80f0-405c59c29152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [73]\n"
     ]
    }
   ],
   "source": [
    "#10.\n",
    "print(f\"Result Set: {boolean_query(\"pattern\", \"AND\", \"clustering\", \"AND\", \"heart\")}\") #Matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa81d54-8316-423e-96db-85c1df5f70ad",
   "metadata": {},
   "source": [
    "## Implementing Proximity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98b8ab75-27d6-44e7-840e-a892e824f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_query(term1, term2, k, positional_index):\n",
    "    \"\"\"Finds documents where term1 and term2 appear within k positions of each other. so k+1 positions apart of each other\n",
    "    Queries of the type X Y / 3 represents doc that contains both X and Y and 3 words apart.\"\"\"\n",
    "    \n",
    "    #Stemming terms to match stored format\n",
    "    term1 = stemmer.stem(term1)\n",
    "    term2 = stemmer.stem(term2)\n",
    "\n",
    "    #Get document position lists for each term from positional index that we have created previously\n",
    "    if term1 not in positional_index or term2 not in positional_index:\n",
    "        return []  #If either term is missing matching is not possible\n",
    "\n",
    "    docs_term1 = positional_index[term1]\n",
    "    docs_term2 = positional_index[term2]\n",
    "\n",
    "    matching_docs = []\n",
    "\n",
    "    #Check for common documents containing both terms so that we check for positions in those documents only which contain both terms.\n",
    "    common_docs = set(docs_term1.keys()) & set(docs_term2.keys())\n",
    "\n",
    "    for doc in common_docs:\n",
    "        positions1 = docs_term1[doc]  #Positions of term1\n",
    "        positions2 = docs_term2[doc]  #Positions of term2\n",
    "\n",
    "        #Two-pointer technique to check if any position satisfies the proximity condition\n",
    "        i, j = 0, 0\n",
    "        while i < len(positions1) and j < len(positions2):\n",
    "            if abs(positions1[i] - positions2[j]) <= k+1:\n",
    "                matching_docs.append(int(doc.replace('.txt', ''))) #Add document to list as terms appear within k of each other.\n",
    "                break  #If one match is found, we don't need to check further\n",
    "            elif positions1[i] < positions2[j]: #if i is behind move i forward or else move j forward\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "\n",
    "    return sorted(matching_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d97e1909-09b0-483d-bb6c-c14a5c4d72f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [40, 54, 110, 111, 112, 113, 158, 163, 173, 180, 181, 202, 220, 237, 238, 239, 240, 258, 277, 283, 295, 305, 350, 405, 421, 437, 438]\n"
     ]
    }
   ],
   "source": [
    "#Checking if it is working\n",
    "result = proximity_query(\"time\", \"series\", 3, positional_index)\n",
    "print(\"Result Set:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555f1d6-b291-4bf3-9837-18b794efb3a4",
   "metadata": {},
   "source": [
    "## Checking Golden Queries for Proximity Queries (Query Number 11 and 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "715bc66b-0e26-4669-9cd0-e500bea9122b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [26]\n"
     ]
    }
   ],
   "source": [
    "#11.\n",
    "result = proximity_query(\"neural\", \"information\", 2, positional_index)\n",
    "print(\"Result Set:\", result) #Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a5fa6b73-337a-4ccd-83a3-b192ac547aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Set: [13, 212]\n"
     ]
    }
   ],
   "source": [
    "#12.\n",
    "result = proximity_query(\"feature\", \"track\", 5, positional_index)\n",
    "print(\"Result Set:\", result) #Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e428d2a-5f8f-4e9f-94e3-182c4feed200",
   "metadata": {},
   "source": [
    "## Working on the GUI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb84a7f-b115-4e37-beff-e900687ae3ff",
   "metadata": {},
   "source": [
    "The GUI has been implemented in separate .py files. run app.py from the command prompt using 'streamlit run app.py'\n",
    "Make sure the files '4004Assignment1.ipynb', 'app.py', 'inverted_index.json', 'positional_index.json' and 'search_engine.py'\n",
    "are present in the same directory. Navigate to the directory which contains these files from the command prompt using the cd command\n",
    "and then write streamlit run app.py."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
